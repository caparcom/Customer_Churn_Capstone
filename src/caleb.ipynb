{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d6921",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from utils import db_connect\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "engine = db_connect()\n",
    "\n",
    "# bring in the data\n",
    "main_df = pd.read_sql('SELECT * FROM combined_data', con=engine)\n",
    "\n",
    "# function to mark whether a school was operational all five years of data\n",
    "# creates our 'Currently operational' feature\n",
    "def mark_always_operational(main_df):\n",
    "    \n",
    "    total_years = main_df['SURVYEAR'].nunique()\n",
    "    \n",
    "    main_df['SY_STATUS_TEXT'] = main_df['SY_STATUS_TEXT'].str.strip()\n",
    "    \n",
    "    operational_counts = (\n",
    "        main_df[main_df['SY_STATUS_TEXT'] == 'Currently operational']\n",
    "        .groupby('NCESSCH')['SURVYEAR']\n",
    "        .nunique()\n",
    "    )\n",
    "    \n",
    "    always_operational_schools = operational_counts[operational_counts == total_years].index\n",
    "    \n",
    "    main_df['concurrently_operational'] = main_df['NCESSCH'].isin(always_operational_schools)\n",
    "    \n",
    "    return main_df\n",
    "\n",
    "# apply the function\n",
    "mark_always_operational(main_df=main_df)\n",
    "\n",
    "# drop records that were not fully operational across all five years\n",
    "main_df = main_df[main_df['concurrently_operational'] != False]\n",
    "\n",
    "# strip whitespace\n",
    "for col in main_df.select_dtypes(include=[\"string\"]).columns:\n",
    "    main_df[col] = main_df[col].str.strip()\n",
    "\n",
    "# begin defining data type conversion processes\n",
    "# Change columns to floats\n",
    "float_cols = [\"X\", \"Y\", \"LATCOD\", \"LONCOD\", \"FTE\", \"STUTERATIO\"]\n",
    "\n",
    "# change columns to int\n",
    "int_cols = [\n",
    "    \"OBJECTID\", \"GSLO\", \"GSHI\",\n",
    "    \"TOTFRL\", \"FRELCH\", \"REDLCH\", \"DIRECTCERT\",\n",
    "    \"PK\", \"KG\", \"G01\", \"G02\", \"G03\", \"G04\", \"G05\", \"G06\",\n",
    "    \"G07\", \"G08\", \"G09\", \"G10\", \"G11\", \"G12\", \"G13\",\n",
    "    \"UG\", \"AE\",\n",
    "    \"TOTMENROL\", \"TOTFENROL\", \"TOTAL\", \"MEMBER\",\n",
    "    \"AMALM\", \"AMALF\", \"AM\",\n",
    "    \"ASALM\", \"ASALF\", \"AS\",\n",
    "    \"BLALM\", \"BLALF\", \"BL\",\n",
    "    \"HPALM\", \"HPALF\", \"HP\",\n",
    "    \"HIALM\", \"HIALF\", \"HI\",\n",
    "    \"TRALM\", \"TRALF\", \"TR\",\n",
    "    \"WHALM\", \"WHALF\", \"WH\"\n",
    "]\n",
    "\n",
    "# Change columns to strings\n",
    "string_cols = [\n",
    "    \"NCESSCH\", \"SURVYEAR\", \"STABR\", \"LEAID\", \"ST_LEAID\",\n",
    "    \"LEA_NAME\", \"SCH_NAME\",\n",
    "    \"LSTREET1\", \"LSTREET2\", \"LCITY\", \"LSTATE\",\n",
    "    \"LZIP\", \"LZIP4\", \"PHONE\",\n",
    "    \"VIRTUAL\", \"SCHOOL_LEVEL\", \"SCHOOL_TYPE_TEXT\",\n",
    "    \"STATUS\", \"SY_STATUS_TEXT\", \"ULOCALE\", \"NMCNTY\",\n",
    "    \"CHARTER_TEXT\", \"LSTREET3\", \"TITLEI\", \"STITLEI\", \"MAGNET_TEXT\"\n",
    "]\n",
    " \n",
    "# -1 or M -> Indicates that the data are missing.\n",
    "\n",
    "# -2 or N -> Indicates that the data are not applicable.\n",
    "\n",
    "# -9 -> Indicates that the data do not meet NCES data quality standards.\n",
    "\n",
    "# function to clean NCES error codes\n",
    "def clean_nces_error_codes(main_df, cols):\n",
    "    error_values = [\"M\", \"-1\", \"-9\", \"Missing\", -1, -9\n",
    "]\n",
    "    main_df[cols] = main_df[cols].replace(error_values, np.nan)\n",
    "    return main_df\n",
    "\n",
    "# clean ALL columns \n",
    "cols = float_cols + int_cols + string_cols\n",
    "main_df = clean_nces_error_codes(main_df, cols)\n",
    "\n",
    "# convert floats safely\n",
    "for col in float_cols:\n",
    "    main_df[col] = pd.to_numeric(main_df[col], errors=\"coerce\")\n",
    "\n",
    "    # convert ints safely\n",
    "for col in int_cols:\n",
    "    main_df[col] = pd.to_numeric(main_df[col], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "    # convert strings\n",
    "for col in string_cols:\n",
    "    main_df[col] = main_df[col].astype(\"string\")\n",
    "\n",
    "    # round coordinates\n",
    "main_df[\"LATCOD\"] = main_df[\"LATCOD\"].round(4)\n",
    "main_df[\"LONCOD\"] = main_df[\"LONCOD\"].round(4)\n",
    "\n",
    "# extract start Year - convert to int for sorting\n",
    "main_df['SURVYEAR'] = main_df['SURVYEAR'].str[:4].astype(int)\n",
    "\n",
    "# removing virtual schools\n",
    "main_df = main_df[main_df['VIRTUAL'].isin(['Not Virtual', 'Not a virtual school'])]\n",
    "\n",
    "# drop the virtual feature\n",
    "main_df.drop(columns='VIRTUAL', inplace=True)\n",
    "\n",
    "# only keeping 'regular' public schools, removing: [ 'Career and Technical School',\n",
    "# 'Special education school', 'Alternative Education School',\n",
    "# 'Alternative/other school', 'Vocational school']\n",
    "main_df = main_df[main_df['SCHOOL_TYPE_TEXT'].isin(['Regular school', 'Regular School'])]\n",
    "\n",
    "# drop the SCHOOL_TYPE_TEXT feature\n",
    "main_df.drop(columns='SCHOOL_TYPE_TEXT', inplace=True)\n",
    "\n",
    "# replace na values with 0\n",
    "main_df = main_df.fillna(0)\n",
    "\n",
    "# Checking records against all five years\n",
    "counts = main_df[\"NCESSCH\"].value_counts()\n",
    "keep_ids = counts[counts == 5].index\n",
    "main_df = main_df[main_df[\"NCESSCH\"].isin(keep_ids)].copy()\n",
    "\n",
    "print(f\"1. main_df shape: {main_df.shape}\")\n",
    "\n",
    "# Simplify ULOCALE\n",
    "main_df[\"locale_category\"] = main_df[\"ULOCALE\"].str.split(\"-\").str[1].str.split(\":\").str[0]\n",
    "\n",
    "# Drop the ULOCALE feature because we now have our simplified locale_category feature\n",
    "main_df.drop(columns='ULOCALE', inplace=True)\n",
    "\n",
    "# Title I rough breakdown:\n",
    "\n",
    "# Participating:\n",
    "# 1 - Yes - School participates in Title I funding / programs\n",
    "# 5 - Title I schoolwide school - ENTIRE school recieves Title I support. Funds can be used for all students\n",
    "# 2 - Title I targeted assistance school - Only SPECIFIC eligible students recieve services (usually low-income or academically at risk)\n",
    "\n",
    "# Eligible, but no program running:\n",
    "# 4 - Title I schoolwide eligible school - no program - Enough low-income students to qualify for schoolwide funding, but not using it\n",
    "# 1 - Title I targeted assistance eligible school - No program - Eligible for targeted assistance but not participating\n",
    "\n",
    "# Hybrid\n",
    "# 3 Title I schoolwide eligible - Title I targeted assitance program - School qualifies for schoolwide funding but has chosen to run only a targeted program\n",
    "\n",
    "# Explicit non-participation\n",
    "# 2 - No - School does not participate\n",
    "# 6 - Not a Title I school\n",
    "\n",
    "# 0\n",
    "# 0 - Assuming missing, unknown, or not reported\n",
    "\n",
    "\n",
    "# Conceptual differences:\n",
    "# Schoolwide = whole school qualifies = High funding flexibility - Typical poverty threshold >= 40% low-income\n",
    "# Targeted = only some students qualify = Limited funding flexibility - lower threshold for poverty\n",
    "\n",
    "# standardize TITLEI\n",
    "schoolwide = ['1-Yes', '5-Title I schoolwide school']\n",
    "targeted = ['2-Title I targeted assistance school', '3-Title I schoolwide eligible-Title I targeted assistance program']\n",
    "elig_no_participate = ['4-Title I schoolwide eligible school-No program', \n",
    "                       '1-Title I targeted assistance eligible school-No program']\n",
    "not_elig = ['2-No', '6-Not a Title I school']\n",
    "missing = [0]\n",
    "\n",
    "def group_titlei(col_TITLEI):\n",
    "    if col_TITLEI in missing:\n",
    "        return \"Unknown\"\n",
    "    elif col_TITLEI in schoolwide:\n",
    "        return \"Schoolwide\"\n",
    "    elif col_TITLEI in targeted:\n",
    "        return \"Targeted\"\n",
    "    elif col_TITLEI in elig_no_participate:\n",
    "        return \"Eligible_No_Program\"\n",
    "    elif col_TITLEI in not_elig:\n",
    "        return \"Not_Eligible\"\n",
    "    else:\n",
    "        return \"Error\"\n",
    "    \n",
    "# apply the above function to main_df\n",
    "main_df['TITLEI_GROUPED'] = main_df['TITLEI'].apply(group_titlei)\n",
    "\n",
    "# standardize STITLEI\n",
    "STITLEI_yes = ['1-Yes', 'Yes']\n",
    "STITLEI_no = ['2-No', 'No']\n",
    "STITLEI_unknown = [0]\n",
    "\n",
    "def standardize_STITLEI(col_STITLEI):\n",
    "    if col_STITLEI in STITLEI_yes:\n",
    "        return 'Yes'\n",
    "    elif col_STITLEI in STITLEI_no:\n",
    "        return 'No'\n",
    "    elif col_STITLEI in STITLEI_unknown:\n",
    "        return 'Unknown'\n",
    "    else:\n",
    "        return 'Error'\n",
    "    \n",
    "# apply the above function to main_df    \n",
    "main_df['STITLEI'] = main_df['STITLEI'].apply(standardize_STITLEI)\n",
    "\n",
    "# update the contradticions between TITLEI and STITLEI (Updating the below to 'Targeted' group instead of 'Schoolwide')\n",
    "main_df.loc[(main_df['TITLEI'] == '1-Yes') & (main_df['STITLEI'] == 'No'), 'TITLEI_GROUPED'] = 'Targeted'\n",
    "\n",
    "# Checking records against all five years\n",
    "counts = main_df[\"NCESSCH\"].value_counts()\n",
    "keep_ids = counts[counts == 5].index\n",
    "main_df = main_df[main_df[\"NCESSCH\"].isin(keep_ids)].copy()\n",
    "\n",
    "print(f\"2. main_df shape: {main_df.shape}\")\n",
    "\n",
    "# further filtering on positive student teacher ratios\n",
    "main_df = main_df[main_df['STUTERATIO'] != 0.0]\n",
    "\n",
    "# define additional redundant columns\n",
    "redundant_cols = ['X', 'Y', 'OBJECTID', 'ST_LEAID', 'LSTREET1', 'LSTREET2', 'LSTREET3', \n",
    "                  'LZIP4', 'PHONE', 'AMALM', 'AMALF', 'ASALM', 'ASALF', \n",
    "                  'BLALM', 'BLALF', 'HPALM', 'HPALF', 'HIALM', 'HIALF', 'TRALM', 'TRALF', \n",
    "                  'WHALM', 'WHALF', 'STABR', 'LCITY', 'LSTATE', 'LZIP', 'SCHOOL_LEVEL', 'GSLO', 'GSHI'\n",
    "                  , 'STATUS', 'SY_STATUS_TEXT', 'NMCNTY', 'DIRECTCERT', 'AE', 'TOTFENROL', 'TOTMENROL',\n",
    "                  'concurrently_operational', 'TITLEI', 'STITLEI', 'MEMBER']\n",
    "\n",
    "# drop additional redundant cols\n",
    "main_df = main_df.drop(columns=redundant_cols)\n",
    "\n",
    "# remove the large Alaska homeschool support program from data set\n",
    "main_df[main_df['NCESSCH'] != '20013000253']\n",
    "\n",
    "# trim the top percentile off\n",
    "def trim_top_percentile(df, col=\"STUTERATIO\", percentile=0.99):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate cutoff\n",
    "    cutoff = df[col].quantile(percentile)\n",
    "\n",
    "    # Count rows before trimming\n",
    "    before_count = df.shape[0]\n",
    "\n",
    "    # Trim\n",
    "    df_trimmed = df[df[col] <= cutoff].copy()\n",
    "\n",
    "    after_count = df_trimmed.shape[0]\n",
    "\n",
    "    print(f\"{percentile*100}th percentile cutoff: {cutoff:.2f}\")\n",
    "    print(f\"Rows before: {before_count}\")\n",
    "    print(f\"Rows after: {after_count}\")\n",
    "    print(f\"Rows removed: {before_count - after_count}\")\n",
    "\n",
    "    print(\"\\nTop values after trimming:\")\n",
    "    print(\n",
    "        df_trimmed.sort_values(col, ascending=False)[\n",
    "            [\"NCESSCH\", \"SURVYEAR\", \"FTE\", col]\n",
    "        ].head(10)\n",
    "    )\n",
    "\n",
    "    return df_trimmed\n",
    "\n",
    "# apply the above function to main_df\n",
    "main_df = trim_top_percentile(main_df, col=\"STUTERATIO\", percentile=0.99)\n",
    "\n",
    "# trim the bottom percentile off\n",
    "def trim_bottom_percentile(df, col=\"STUTERATIO\", percentile=0.01):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    # Calculate cutoff\n",
    "    cutoff = df[col].quantile(percentile)\n",
    "\n",
    "    # Count rows before trimming\n",
    "    before_count = df.shape[0]\n",
    "\n",
    "    # Trim bottom values\n",
    "    df_trimmed = df[df[col] >= cutoff].copy()\n",
    "\n",
    "    after_count = df_trimmed.shape[0]\n",
    "\n",
    "    print(f\"{percentile*100}th percentile cutoff: {cutoff:.2f}\")\n",
    "    print(f\"Rows before: {before_count}\")\n",
    "    print(f\"Rows after: {after_count}\")\n",
    "    print(f\"Rows removed: {before_count - after_count}\")\n",
    "\n",
    "    print(\"\\nBottom values after trimming:\")\n",
    "    print(\n",
    "        df_trimmed.sort_values(col, ascending=True)[\n",
    "            [\"NCESSCH\", \"SURVYEAR\", \"FTE\", col]\n",
    "        ].head(10)\n",
    "    )\n",
    "\n",
    "    return df_trimmed\n",
    "\n",
    "main_df = trim_bottom_percentile(main_df)\n",
    "\n",
    "# create our high-strain feature\n",
    "main_df[\"high_strain\"] = (main_df[\"STUTERATIO\"] > 20).astype(int)\n",
    "\n",
    "# updating nces error codes to No or 0 for respective columns\n",
    "values = [\"N\", \"-2\",\"Not applicable\", \"Not Applicable\",-2,]\n",
    "main_df.loc[main_df['CHARTER_TEXT'].isin(values), 'CHARTER_TEXT'] = 'No'\n",
    "main_df.loc[main_df['MAGNET_TEXT'].isin(values), 'MAGNET_TEXT'] = 'No'\n",
    "main_df.loc[main_df['FRELCH'].isin(values), 'FRELCH'] = 0\n",
    "main_df.loc[main_df['REDLCH'].isin(values), 'REDLCH'] = 0\n",
    "\n",
    "# keeping all records with 5 years of data\n",
    "counts = main_df[\"NCESSCH\"].value_counts()\n",
    "keep_ids = counts[counts == 5].index\n",
    "main_df = main_df[main_df[\"NCESSCH\"].isin(keep_ids)].copy()\n",
    "\n",
    "print(f\"3. main_df shape: {main_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94550a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d6709d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd574253",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df = pd.read_sql('SELECT * FROM main_df', con=engine)\n",
    "main_df.to_sql(\"main_df\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25339551",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c596cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=main_df, x='TOTAL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91be04bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['STUTERATIO'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da695c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(data=main_df, x='STUTERATIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cacf08d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# modeling EDA\n",
    "\n",
    "# Do rural schools have higher strain?\n",
    "# Do Title I schools have higher strain?\n",
    "# Does enrollment size predict strain?\n",
    "# Do funding indicators correlate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beb64bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"main_df shape: {main_df.shape}\")\n",
    "print(f\"high strain schools: {main_df[main_df['high_strain'] == 1].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df70202d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e65d319",
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_cols = ['SURVYEAR', 'LEAID', 'CHARTER_TEXT', 'MAGNET_TEXT', 'TOTFRL', 'FRELCH', 'REDLCH', \n",
    "             'PK', 'KG', 'G01', 'G02', 'G03', 'G04', 'G05', 'G06', 'G07', 'G08',\n",
    "               'G09', 'G10', 'G11', 'G12', 'G13', 'UG', 'TOTAL', 'AM', 'AS', 'BL', 'HP', 'HI', 'TR'\n",
    "               , 'WH', 'LATCOD', 'LONCOD', 'locale_category', 'TITLEI_GROUPED', 'high_strain']\n",
    "refined_df = main_df[keep_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79691528",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for corr\n",
    "subset_cols = ['high_strain', 'LEAID', 'TOTAL', 'locale_category', 'TITLEI_GROUPED']\n",
    "subset_df = main_df[subset_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9785471",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_cols = ['TOTAL', 'AM', 'AS', 'BL', 'HP', 'HI', 'TR'\n",
    "               , 'WH', 'LATCOD', 'LONCOD','high_strain', 'TOTFRL']\n",
    "subset_df = main_df[subset_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0f3cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929395eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d17d997",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = subset_df.select_dtypes(include=['category', 'object']).columns\n",
    "\n",
    "# Keep numeric columns\n",
    "num_cols = subset_df.select_dtypes(include=['number']).columns\n",
    "\n",
    "df_encoded_full = pd.get_dummies(subset_df, columns=cat_cols)\n",
    "\n",
    "print(df_encoded_full.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e988373d",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = df_encoded_full.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78974fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, \n",
    "            annot=True,        # show the correlation numbers\n",
    "            fmt=\".2f\",         # 2 decimal places\n",
    "            cmap=\"coolwarm\",   # color scheme\n",
    "            cbar=True)\n",
    "plt.title(\"Correlation Matrix of Categorical Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2d5034",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95cf70",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d57acb",
   "metadata": {},
   "source": [
    "# Logistic Regression preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb25630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ethnicity proportions\n",
    "for col in ['AM','AS','BL','HP','HI','TR','WH']:\n",
    "    main_df[f'prop_{col}'] = main_df[col] / main_df['TOTAL']\n",
    "\n",
    "# Grade-level ratios\n",
    "main_df['upper_grades_ratio'] = main_df[['G09','G10','G11','G12','G13']].sum(axis=1) / main_df['TOTAL']\n",
    "main_df['lower_grades_ratio'] = main_df[['PK','KG','G01','G02','G03','G04','G05']].sum(axis=1) / main_df['TOTAL']\n",
    "main_df['middle_grades_ratio'] = main_df[['G06','G07','G08']].sum(axis=1) / main_df['TOTAL']\n",
    "\n",
    "# Free/reduced lunch ratios\n",
    "main_df['frl_ratio'] = main_df['FRELCH'] / main_df['TOTFRL']\n",
    "main_df['redl_ratio'] = main_df['REDLCH'] / main_df['TOTFRL']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5686e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f211cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = ['AM','AS','BL','HP','HI','TR','WH', 'G06','G07','G08','G09','G10','G11','G12','G13', 'PK','KG','G01','G02','G03','G04','G05', 'TOTFRL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48644e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d6dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8e51b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df['CHARTER_TEXT'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e240d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# main_df.loc[main_df['REDLCH'].isin(values), 'REDLCH'] = 0\n",
    "main_df.loc[main_df['CHARTER_TEXT'] == 0, 'CHARTER_TEXT'] = 'Missing'\n",
    "main_df.loc[main_df['MAGNET_TEXT'] == 0, 'MAGNET_TEXT'] = 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfceec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0536d29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4e6092",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.drop(columns=['NCESSCH', 'SURVYEAR', 'LEA_NAME', \n",
    "                      'SCH_NAME', 'FRELCH', 'REDLCH', 'FTE', 'STUTERATIO'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52110790",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=main_df.select_dtypes(include=['object','category']).columns\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88b479a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.get_dummies(\n",
    "    main_df,\n",
    "    columns=main_df.select_dtypes(include=['object','category']).columns,\n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb78071d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859374c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When I did the free and reduced lunch convertions, it turned 0's into NAs so need to update to 0's\n",
    "main_df.loc[main_df['frl_ratio'].isna(), 'frl_ratio'] = 0\n",
    "main_df.loc[main_df['redl_ratio'].isna(), 'redl_ratio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c889a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = main_df.drop(columns=[\"high_strain\"])\n",
    "y = main_df[\"high_strain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9513b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db919aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unsure if scaling is the correct path with this dataset... might revist this with scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1a4570",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df[main_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5970947",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583fbf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline logreg model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "base_model = LogisticRegression(max_iter=500)\n",
    "base_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e947712",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_y_pred = base_model.predict(X_test)\n",
    "base_model_y_prob = base_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a01ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating base logreg model\n",
    "# as thought, this model is essentially guessing, with a horrible recall of actually determining strain\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, classification_report, confusion_matrix\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, base_model_y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, base_model_y_prob))\n",
    "print(classification_report(y_test, base_model_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943bd126",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    \"C\": [0.01, 0.1, 1, 10],\n",
    "    \"penalty\": [\"l2\"],\n",
    "    \"solver\": [\"lbfgs\", \"liblinear\"]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    LogisticRegression(class_weight='balanced', max_iter=500),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"roc_auc\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8fe2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_model = grid.best_estimator_\n",
    "print(grid.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553159c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_y_pred = cv_model.predict(X_test)\n",
    "cv_y_prob = cv_model.predict_proba(X_test)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373a17e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating best model from cv \n",
    "print(\"Accuracy:\", accuracy_score(y_test, cv_y_pred))\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, cv_y_prob))\n",
    "print(classification_report(y_test, cv_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747940b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df = pd.DataFrame({\n",
    "    \"feature\": X.columns,\n",
    "    \"coef\": cv_model.coef_[0]\n",
    "}).sort_values(\"coef\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8981293a",
   "metadata": {},
   "outputs": [],
   "source": [
    "coef_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e012ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpretation to come of the above ceofficients, I still want to try \n",
    "# to scale some things out as well and see if our coefficients change at all.\n",
    "# moving to looking at additional data to bring in."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc9e7e3",
   "metadata": {},
   "source": [
    "Looking at additional data set for income information broken out by LEAID (school district)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a205a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df = pd.read_csv('../data/raw/DP03_001_USSchoolDistrictAll_219154124322.txt', sep=\"|\", dtype=str)\n",
    "\n",
    "print(addi_df.head())\n",
    "print(addi_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240e6525",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e340215e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# econ_df = pd.read_sql('SELECT * FROM econ_2018_2022_rolling', con=engine)\n",
    "addi_df.to_sql(\"econ_2018_2022_rolling\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa19dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c40dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://api.census.gov/data/2022/acs/acs5/profile/groups/DP03.json\"\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "print(response.status_code)  # Should be 200\n",
    "print(response.headers.get('Content-Type'))  # Should indicate JSON\n",
    "\n",
    "if response.status_code == 200:\n",
    "    try:\n",
    "        data = response.json()\n",
    "        print(data.keys())  # Should show keys like 'variables', 'name', 'description'\n",
    "    except ValueError as e:\n",
    "        print(\"JSON decode error:\", e)\n",
    "        print(\"Response text preview:\", response.text[:500])\n",
    "else:\n",
    "    print(\"Request failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a4c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c59cb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = data['variables']\n",
    "\n",
    "# Convert to DataFrame\n",
    "var_df = pd.DataFrame.from_dict(variables, orient='index').reset_index()\n",
    "var_df = var_df.rename(columns={'index': 'variable'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521e629c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bc2125",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb99fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# econ_var_def_df = pd.read_sql('SELECT * FROM econ_var_def_2018_2022_rolling', con=engine)\n",
    "var_df.to_sql(\"econ_var_def_2018_2022_rolling\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31b2151",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983cbd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_cols = addi_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8757b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(addi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e24140",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_cols = pd.Series(addi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ec558c",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(addi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa934d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_cols = list(addi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7211742",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(addi_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb8e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df[var_df['variable'].isin(addi_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b3e617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After some research the initial data set contained the non-official, human firendly col names\n",
    "# however the API call to gather the definitions are the official labels, meaning we have a mismatch,\n",
    "# trying to determine a solid option for convertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a86c531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# suppose addi_cols = ['DP03_133pct', 'DP03_133pctmoe', ...]\n",
    "mapped_cols = []\n",
    "for col in addi_cols:\n",
    "    # extract the number part\n",
    "    m = re.search(r'DP03_(\\d+)', col)\n",
    "    if m:\n",
    "        rownum = m.group(1).zfill(4)  # API variables are zero-padded 4 digits\n",
    "        if col.endswith('pct'):\n",
    "            mapped_cols.append(f'DP03_{rownum}PE')\n",
    "        elif col.endswith('pctmoe'):\n",
    "            mapped_cols.append(f'DP03_{rownum}M')\n",
    "            \n",
    "print(mapped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a996ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df[var_df['variable'].isin(mapped_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15fc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Success!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2e6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4db7c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df[var_df['variable'].isin(mapped_cols)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88fbdac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# addi_df = the new economic data set (2018-2022 rolling 5 year)\n",
    "# var_df = API call to the data dictionary for the DP03 economic variable list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c94ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa12027",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850d0f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df['DP03_0053EA'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9989cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I actually want the inverse idea from above, to be able to bring addi_df into the db\n",
    "def api_to_addi(api_cols):\n",
    "    converted = []\n",
    "    \n",
    "    for col in api_cols:\n",
    "        m = re.search(r'DP03_(\\d+)(PE|M)$', col)\n",
    "        if m:\n",
    "            num = str(int(m.group(1)))  # remove zero padding\n",
    "            suffix = m.group(2)\n",
    "            \n",
    "            if suffix == \"PE\":\n",
    "                converted.append(f\"DP03_{num}pct\")\n",
    "            elif suffix == \"M\":\n",
    "                converted.append(f\"DP03_{num}pctmoe\")\n",
    "    \n",
    "    return converted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37b351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df['label'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add3196",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_vars = var_df[\n",
    "    var_df['label'].str.contains('income', case=False, na=False)\n",
    "]['variable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87998910",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_vars = list(income_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f438574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "income_vars = api_to_addi(income_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab6a68b",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7023772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_remove = [\n",
    "    'DP03_71pct', 'DP03_69pctmoe', 'DP03_67pctmoe', 'DP03_63pctmoe',\n",
    "    'DP03_65pctmoe', 'DP03_62pctmoe', 'DP03_67pct', 'DP03_90pct',\n",
    "    'DP03_73pctmoe', 'DP03_75pctmoe', 'DP03_71pctmoe', 'DP03_51pct',\n",
    "    'DP03_88pctmoe', 'DP03_89pctmoe', 'DP03_86pctmoe', 'DP03_87pctmoe',\n",
    "    'DP03_93pctmoe', 'DP03_94pctmoe', 'DP03_91pctmoe', 'DP03_92pctmoe',\n",
    "    'DP03_90pctmoe', 'DP03_65pct', 'DP03_88pct', 'DP03_62pct',\n",
    "    'DP03_75pct', 'DP03_93pct', 'DP03_89pct', 'DP03_63pct',\n",
    "    'DP03_94pct', 'DP03_86pct', 'DP03_73pct', 'DP03_91pct',\n",
    "    'DP03_69pct', 'DP03_87pct', 'DP03_92pct', 'DP03_51pctmoe'\n",
    "]\n",
    "\n",
    "income_vars = [c for c in income_vars if c not in cols_to_remove]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82485ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df[income_vars].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c86122",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df[var_df['variable'].isin(income_vars)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3974823",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6115fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point I need to just build a crossover mapping table between the two\n",
    "import re\n",
    "rows = []\n",
    "\n",
    "for var in var_df['variable']:\n",
    "    m = re.match(r'DP03_(\\d+)(PE|M)$', var)\n",
    "    if m:\n",
    "        num = str(int(m.group(1)))\n",
    "        suffix = m.group(2)\n",
    "\n",
    "        if suffix == \"PE\":\n",
    "            addi = f\"DP03_{num}pct\"\n",
    "        else:\n",
    "            addi = f\"DP03_{num}pctmoe\"\n",
    "\n",
    "        rows.append({\n",
    "            \"api_var\": var,\n",
    "            \"addi_col\": addi,\n",
    "            \"label\": var_df.loc[var_df.variable == var, \"label\"].values[0],\n",
    "            \"concept\": var_df.loc[var_df.variable == var, \"concept\"].values[0]\n",
    "        })\n",
    "\n",
    "map_df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef0b3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba004418",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_df.to_sql(\"acs_column_map\", engine, if_exists=\"replace\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3facfa51",
   "metadata": {},
   "outputs": [],
   "source": [
    "addi_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebf6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['DP03_48pct','DP03_22pct','DP03_111pct','DP03_35pct','DP03_66pct','DP03_79pct','DP03_53pct','DP03_97pct','DP03_40pct','DP03_71pct','DP03_84pct','DP03_107pct','DP03_5pct','DP03_125pct','DP03_18pct','DP03_49pct','DP03_36pct','DP03_112pct','DP03_23pct','DP03_67pct','DP03_10pct','DP03_41pct','DP03_130pct','DP03_54pct','DP03_85pct','DP03_98pct','DP03_108pct','DP03_72pct','DP03_19pct','DP03_113pct','DP03_6pct','DP03_90pct','DP03_126pct','DP03_59pct','DP03_2pct','DP03_135pct','DP03_46pct','DP03_122pct','DP03_20pct','DP03_33pct','DP03_64pct','DP03_77pct','DP03_118pct','DP03_51pct','DP03_105pct','DP03_52pct','DP03_95pct','DP03_136pct','DP03_29pct','DP03_82pct','DP03_3pct','DP03_16pct','DP03_110pct','DP03_47pct','DP03_123pct','DP03_34pct','DP03_78pct','DP03_21pct','DP03_65pct','DP03_106pct','DP03_83pct','DP03_119pct','DP03_96pct','DP03_137pct','DP03_17pct','DP03_70pct','DP03_124pct','DP03_4pct','DP03_133pct','DP03_13pct','DP03_120pct','DP03_57pct','DP03_44pct','DP03_88pct','DP03_31pct','DP03_62pct','DP03_9pct','DP03_129pct','DP03_75pct','DP03_93pct','DP03_116pct','DP03_27pct','DP03_80pct','DP03_103pct','DP03_1pct','DP03_121pct','DP03_14pct','DP03_45pct','DP03_134pct','DP03_58pct','DP03_89pct','DP03_32pct','DP03_76pct','DP03_117pct','DP03_63pct','DP03_50pct','DP03_28pct','DP03_81pct','DP03_94pct','DP03_104pct','DP03_15pct','DP03_24pct','DP03_37pct','DP03_68pct','DP03_100pct','DP03_11pct','DP03_55pct','DP03_109pct','DP03_131pct','DP03_99pct','DP03_42pct','DP03_86pct','DP03_7pct','DP03_60pct','DP03_73pct','DP03_114pct','DP03_101pct','DP03_127pct','DP03_38pct','DP03_91pct','DP03_132pct','DP03_25pct','DP03_69pct','DP03_12pct','DP03_43pct','DP03_56pct','DP03_87pct','DP03_30pct','DP03_74pct','DP03_128pct','DP03_61pct','DP03_8pct','DP03_92pct','DP03_102pct','DP03_26pct','DP03_115pct','DP03_39pct']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef1a453",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01728713",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744776c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from utils import db_connect\n",
    "engine = db_connect()\n",
    "main_econ_df = pd.read_sql('SELECT * FROM main_econ_view', con=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c7bea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df['LEAID'] = main_econ_df['LEAID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a11d3fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df['SURVYEAR'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77222485",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06313e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREPROCESSING\n",
    "# Ethnicity proportions\n",
    "for col in ['AM','AS','BL','HP','HI','TR','WH']:\n",
    "    main_econ_df[f'prop_{col}'] = main_econ_df[col] / main_df['TOTAL']\n",
    "\n",
    "# Grade-level ratios\n",
    "main_econ_df['upper_grades_ratio'] = main_econ_df[['G09','G10','G11','G12','G13']].sum(axis=1) / main_econ_df['TOTAL']\n",
    "main_econ_df['lower_grades_ratio'] = main_econ_df[['PK','KG','G01','G02','G03','G04','G05']].sum(axis=1) / main_econ_df['TOTAL']\n",
    "main_econ_df['middle_grades_ratio'] = main_econ_df[['G06','G07','G08']].sum(axis=1) / main_econ_df['TOTAL']\n",
    "\n",
    "# Free/reduced lunch ratios\n",
    "main_econ_df['frl_ratio'] = main_econ_df['FRELCH'] / main_econ_df['TOTFRL']\n",
    "main_econ_df['redl_ratio'] = main_econ_df['REDLCH'] / main_econ_df['TOTFRL']\n",
    "\n",
    "cols_to_drop = ['AM','AS','BL','HP','HI','TR','WH', 'G06','G07','G08','G09','G10','G11','G12','G13', 'PK','KG','G01','G02','G03','G04','G05', 'TOTFRL']\n",
    "\n",
    "main_econ_df.drop(columns=cols_to_drop, inplace=True)\n",
    "\n",
    "main_econ_df.loc[main_econ_df['CHARTER_TEXT'] == 0, 'CHARTER_TEXT'] = 'Missing'\n",
    "main_econ_df.loc[main_econ_df['MAGNET_TEXT'] == 0, 'MAGNET_TEXT'] = 'Missing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf82b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1be174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df.drop(columns=['NCESSCH', 'SURVYEAR', 'LEA_NAME', \n",
    "                      'SCH_NAME', 'FRELCH', 'REDLCH', 'FTE', 'STUTERATIO'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ab417c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols=main_econ_df.select_dtypes(include=['object','category']).columns\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c03f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df = pd.get_dummies(\n",
    "    main_econ_df,\n",
    "    columns=main_econ_df.select_dtypes(include=['object','category']).columns,\n",
    "    drop_first=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7211c835",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df.loc[main_econ_df['frl_ratio'].isna(), 'frl_ratio'] = 0\n",
    "main_econ_df.loc[main_econ_df['redl_ratio'].isna(), 'redl_ratio'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc50305",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = main_econ_df.drop(columns=[\"high_strain\"])\n",
    "y = main_econ_df[\"high_strain\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927e3b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a9fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_econ_df[main_econ_df.isna().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32792c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# organization\n",
    "# visualizations\n",
    "# figure out tableau (or streamlit)\n",
    "# go through presentation of ideas (what does it all mean)\n",
    "# double down on API files iteration\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
